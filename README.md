# VNStock Automated Analytics & Big Data Pipeline ğŸ“ˆ

An end-to-end Data Engineering project that automates the ingestion, processing, and analysis of Vietnamese stock market data (VN30) using the Modern Data Stack.

## ğŸ— System Architecture

The pipeline is designed to handle daily/yearly batch processing with a hybrid cloud-local approach:



1.  **Orchestration**: Managed by **Apache Airflow** (running via **Astro CLI**).
2.  **Data Ingestion**: Fetches stock data from **Vnstock API** for 29 tickers.
3.  **Storage (Data Lake)**: Raw and processed data stored in **Google Cloud Storage (GCS)**.
4.  **Compute (Big Data)**: Triggers distributed **PySpark** jobs on **Google Cloud Dataproc** for heavy transformations.
5.  **Analytics Layer**: Aggregates financial KPIs (Volatility, Total Return) and stores them in **PostgreSQL**.
6.  **Containerization**: Entire environment isolated using **Docker**.

## ğŸš€ Key Features

- **Automated ETL**: Daily ingestion schedules with Airflow DAGs.
- **Stock Recommendation Engine**: Built-in logic using Pandas/PySpark to calculate investment signals (Strong Buy, Hold, etc.).
- **Data Idempotency**: Implemented `UPSERT` logic in PostgreSQL to prevent duplicate records during re-runs.
- **Cloud Integration**: Uses IAM Service Accounts for secure local-to-cloud connectivity.

## ğŸ›  Tech Stack

| Category | Technology |
| :--- | :--- |
| **Orchestration** | Apache Airflow (Astro CLI) |
| **Languages** | Python, PySpark, SQL |
| **Cloud (GCP)** | GCS, Dataproc, IAM |
| **Database** | PostgreSQL |
| **Infrastructure** | Docker |
| **Libraries** | Pandas, Vnstock API, Psycopg2 |

## ğŸ“‚ Project Structure

```text
â”œâ”€â”€ dags/                   # Airflow DAG definitions
â”œâ”€â”€ include/                # Helper scripts and SQL queries
â”œâ”€â”€ plugins/                # Custom Airflow operators/plugins
â”œâ”€â”€ scripts/                # PySpark transformation scripts
â”œâ”€â”€ docker-compose.yaml     # Container orchestration
â””â”€â”€ Dockerfile              # Custom Airflow image with dependencies

```

## âš™ï¸ Getting Started

### Prerequisites

* Docker & Docker Compose
* Astro CLI
* A GCP Account (Service Account JSON key required)

### Setup

1. Clone the repository:
```bash
git clone [https://github.com/tunbq21/airflow_astro_dev.git](https://github.com/tunbq21/airflow_astro_dev.git)
cd airflow_astro_dev

```


2. Set up environment variables in `.env`:
```bash
GCP_CONN_ID='google_cloud_default'
POSTGRES_CONN_ID='postgres_default'

```


3. Start the pipeline:
```bash
astro dev start

```


4. Access Airflow UI at `http://localhost:8080`.

## ğŸ“Š Analytics Output

The pipeline generates a `stock_recommendations` table in PostgreSQL with the following schema:

* `ticker`: Stock symbol (e.g., FPT, VNM)
* `volatility`: Standard deviation of returns
* `total_return`: Percentage growth over period
* `signal`: Recommended action (Buy/Sell/Hold)

---

Developed by **Bui Quang Tuan** - *Data Engineer*

2. **Setup:** Náº¿u project cá»§a báº¡n cÃ³ thÃªm cÃ¡c bÆ°á»›c cÃ i Ä‘áº·t Ä‘áº·c thÃ¹ (nhÆ° config Service Account trÃªn GCP), hÃ£y bá»• sung vÃ o má»¥c **Setup**.
3. **Link trong CV:** Sau khi táº¡o file README nÃ y, cÃ¡i "GitHub Link" trong CV cá»§a báº¡n sáº½ trá»Ÿ nÃªn cá»±c ká»³ giÃ¡ trá»‹ vÃ¬ nÃ³ giáº£i thÃ­ch chi tiáº¿t nhá»¯ng gÃ¬ báº¡n Ä‘Ã£ lÃ m.

Báº¡n cÃ³ muá»‘n mÃ¬nh tá»‘i Æ°u thÃªm Ä‘oáº¡n nÃ o trong ná»™i dung nÃ y khÃ´ng?

```
